#!/bin/bash

source .venv/bin/activate
uv pip install ninja packaging
git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention/hopper
FLASH_ATTENTION_DISABLE_BACKWARD=TRUE \
FLASH_ATTENTION_DISABLE_SPLIT=TRUE \
FLASH_ATTENTION_DISABLE_PAGEDKV=TRUE \
FLASH_ATTENTION_DISABLE_APPENDKV=TRUE \
FLASH_ATTENTION_DISABLE_LOCAL=TRUE \
FLASH_ATTENTION_DISABLE_SOFTCAP=TRUE \
FLASH_ATTENTION_DISABLE_PACKGQA=TRUE \
FLASH_ATTENTION_DISABLE_HDIM96=TRUE \
FLASH_ATTENTION_DISABLE_HDIM192=TRUE \
FLASH_ATTENTION_DISABLE_HDIM256=TRUE \
FLASH_ATTENTION_DISABLE_SM80=TRUE \
TORCH_CUDA_ARCH_LIST="90a" \
MAX_JOBS=$(nproc) \
python3 setup.py install



# WAN needs varlen
#FLASH_ATTENTION_DISABLE_VARLEN=TRUE \